#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç”Ÿç‰©åŒ»å­¦çŸ¥è¯†å›¾è°±æ„å»ºDemo
å®Œæ•´å±•ç¤ºï¼šå®ä½“æå– -> å…³ç³»æå– -> ä¸€è‡´æ€§æ£€æŸ¥ -> çŸ¥è¯†å›¾è°±æ„å»º -> å¯è§†åŒ– -> è¯„æµ‹
"""

import os
import sys
import json
import logging
from pathlib import Path
from typing import List, Dict

# æ·»åŠ srcè·¯å¾„
demo_root = Path(__file__).parent.parent
sys.path.insert(0, str(demo_root / "src"))

from extractors.rule_based import BiomeRuleExtractor
from kg.build import BiomeKnowledgeGraph, build_kg_from_jsonl
from kg.visualize import KGVisualizer
from eval.metrics import KnowledgeGraphEvaluator, evaluate_against_gold_standard

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(demo_root / "demo_run.log")
    ]
)
logger = logging.getLogger(__name__)

def parse_input_text(input_file: str) -> List[Dict[str, str]]:
    """è§£æè¾“å…¥æ–‡æœ¬æ–‡ä»¶ï¼Œæå–æ ‡é¢˜å’Œæ‘˜è¦"""
    documents = []
    
    with open(input_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æŒ‰ç©ºè¡Œåˆ†å‰²æ–‡æ¡£
    docs = content.strip().split('\n\n')
    
    for i, doc in enumerate(docs):
        if doc.strip():
            lines = doc.strip().split('\n')
            title = ""
            abstract = ""
            
            for line in lines:
                if line.startswith("Title:"):
                    title = line[6:].strip()
                elif line.startswith("Abstract:"):
                    abstract = line[9:].strip()
            
            if title and abstract:
                documents.append({
                    "pmid": f"demo_{i+1:03d}",
                    "title": title,
                    "abstract": abstract
                })
    
    return documents

def run_extraction_pipeline(input_file: str, output_dir: str) -> str:
    """è¿è¡Œå®Œæ•´çš„æå–æµæ°´çº¿"""
    logger.info("="*60)
    logger.info("å¼€å§‹ç”Ÿç‰©åŒ»å­¦çŸ¥è¯†æå–æµæ°´çº¿")
    logger.info("="*60)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # è§£æè¾“å…¥æ–‡æ¡£
    logger.info("ğŸ“„ è§£æè¾“å…¥æ–‡æ¡£...")
    documents = parse_input_text(input_file)
    logger.info(f"å…±è§£æåˆ° {len(documents)} ä¸ªæ–‡æ¡£")
    
    # åˆå§‹åŒ–æå–å™¨
    logger.info("ğŸ”§ åˆå§‹åŒ–è§„åˆ™æå–å™¨...")
    extractor = BiomeRuleExtractor()
    
    # æå–ç»“æœå­˜å‚¨
    results = []
    extraction_file = os.path.join(output_dir, "extraction_results.jsonl")
    
    # é€ä¸ªæ–‡æ¡£æå–
    logger.info("ğŸ” å¼€å§‹å®ä½“å…³ç³»æå–...")
    for doc in documents:
        pmid = doc["pmid"]
        title = doc["title"]
        abstract = doc["abstract"]
        
        logger.info(f"å¤„ç†æ–‡æ¡£ {pmid}: {title[:50]}...")
        
        # åˆå¹¶æ ‡é¢˜å’Œæ‘˜è¦
        full_text = f"{title}. {abstract}"
        
        # æ‰§è¡Œä¸¤æ­¥æå–
        result = extractor.extract_complete(full_text, pmid)
        results.append(result)
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        with open(extraction_file, 'a', encoding='utf-8') as f:
            f.write(json.dumps(result, ensure_ascii=False) + '\n')
        
        # è¾“å‡ºæ‘˜è¦
        entity_count = (len(result["entities"]["microbes"]) + 
                       len(result["entities"]["metabolites"]))
        relation_count = len(result["relations"])
        logger.info(f"  âœ… {pmid}: {entity_count}ä¸ªå®ä½“, {relation_count}ä¸ªå…³ç³», "
                   f"è´¨é‡åˆ†æ•°: {result['quality_score']}")
    
    logger.info(f"ğŸ‰ æå–å®Œæˆï¼ç»“æœä¿å­˜åˆ°: {extraction_file}")
    return extraction_file

def build_knowledge_graph(extraction_file: str, output_dir: str) -> BiomeKnowledgeGraph:
    """æ„å»ºçŸ¥è¯†å›¾è°±"""
    logger.info("ğŸ—ï¸ æ„å»ºçŸ¥è¯†å›¾è°±...")
    
    kg = build_kg_from_jsonl(extraction_file)
    
    # ä¿å­˜çŸ¥è¯†å›¾è°±
    kg_file = os.path.join(output_dir, "knowledge_graph.json")
    kg.export_to_json(kg_file)
    
    logger.info(f"ğŸ“Š çŸ¥è¯†å›¾è°±å·²ä¿å­˜åˆ°: {kg_file}")
    return kg

def create_visualizations(kg: BiomeKnowledgeGraph, output_dir: str) -> None:
    """åˆ›å»ºå¯è§†åŒ–"""
    logger.info("ğŸ¨ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    
    visualizer = KGVisualizer(kg)
    viz_dir = os.path.join(output_dir, "visualizations")
    
    visualizer.generate_all_visualizations(viz_dir)
    
    # åˆ›å»ºHTMLæŠ¥å‘Š
    html_file = os.path.join(output_dir, "kg_report.html")
    visualizer.create_html_report(html_file)
    
    logger.info(f"ğŸ–¼ï¸ å¯è§†åŒ–å›¾è¡¨å·²ç”Ÿæˆ: {viz_dir}")
    logger.info(f"ğŸ“„ HTMLæŠ¥å‘Š: {html_file}")

def run_evaluation(extraction_file: str, gold_file: str, output_dir: str) -> None:
    """è¿è¡Œè¯„æµ‹"""
    logger.info("ğŸ“Š å¼€å§‹æ€§èƒ½è¯„æµ‹...")
    
    eval_file = os.path.join(output_dir, "evaluation_report.json")
    evaluator = evaluate_against_gold_standard(extraction_file, gold_file, eval_file)
    
    logger.info(f"ğŸ“ˆ è¯„æµ‹æŠ¥å‘Šå·²ä¿å­˜åˆ°: {eval_file}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§¬ ç”Ÿç‰©åŒ»å­¦çŸ¥è¯†å›¾è°±æ„å»ºDemo")
    print("="*60)
    print("åŠŸèƒ½å±•ç¤ºï¼šå…ˆå®ä½“åå…³ç³» + ä¸€è‡´æ€§æ£€æŸ¥ + è¯„æµ‹ + å¯è§†åŒ–")
    print("="*60)
    
    # æ–‡ä»¶è·¯å¾„
    demo_root = Path(__file__).parent.parent
    input_file = demo_root / "data" / "demo_input.txt"
    gold_file = demo_root / "data" / "demo_gold.jsonl"
    output_dir = demo_root / "results"
    
    try:
        # 1. å®ä½“å…³ç³»æå–
        extraction_file = run_extraction_pipeline(str(input_file), str(output_dir))
        
        # 2. æ„å»ºçŸ¥è¯†å›¾è°±
        kg = build_knowledge_graph(extraction_file, str(output_dir))
        
        # 3. åˆ›å»ºå¯è§†åŒ–
        create_visualizations(kg, str(output_dir))
        
        # 4. è¿è¡Œè¯„æµ‹
        run_evaluation(extraction_file, str(gold_file), str(output_dir))
        
        # 5. è¾“å‡ºæœ€ç»ˆæ€»ç»“
        logger.info("ğŸŠ Demoè¿è¡Œå®Œæˆï¼")
        logger.info("ğŸ“ ç»“æœæ–‡ä»¶:")
        logger.info(f"   - æå–ç»“æœ: {extraction_file}")
        logger.info(f"   - çŸ¥è¯†å›¾è°±: {output_dir}/knowledge_graph.json")
        logger.info(f"   - å¯è§†åŒ–: {output_dir}/visualizations/")
        logger.info(f"   - HTMLæŠ¥å‘Š: {output_dir}/kg_report.html")
        logger.info(f"   - è¯„æµ‹æŠ¥å‘Š: {output_dir}/evaluation_report.json")
        
        print("\n" + "="*60)
        print("ğŸ‰ Demoè¿è¡ŒæˆåŠŸå®Œæˆï¼")
        print("è¯·æŸ¥çœ‹resultsæ–‡ä»¶å¤¹ä¸­çš„è¾“å‡ºæ–‡ä»¶")
        print("="*60)
        
    except Exception as e:
        logger.error(f"Demoè¿è¡Œå¤±è´¥: {e}")
        print(f"âŒ è¿è¡Œå¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
