# BioRelation增强实施计划

本文档概述了BioRelation平台的综合实施计划，该平台旨在从科学文献中提取、组织和实现微生物与代谢物之间关系的查询。此计划针对学术研究环境进行了优化，采用了适当的资源分配策略。

## 第1阶段：数据收集和预处理（3周）

### 1.1 PubMed数据获取系统（1周）
- **目标**：高效收集400,000+篇相关科学论文及其综合元数据
- **实施细节**：
  - **搜索查询实现**：实现综合搜索查询，结合：
    - **微生物术语**：Bacter*, Bacteri*, Bacteroid*, Bacilli, Fung*, Fungi, Fungus, Archaea*, Microb*, Microorganism*, Prokaryot*, Eukaryot*, Pathogen*, "microbial community", "gut flora", "intestinal flora", "gut bacteria", "intestinal microbiota", "gut microbiota", "microbiome", "host-microbiome interaction", Lactobacill*, Clostrid*, Firmicut*, "Escherichia coli", "E. coli", Proteobacteri*, Akkermansi*, Prevotell*, Bifidobacteri*, Lactococc*, Streptococc*, Enterococc*, "Gut-Brain Axis", Yeast*, Candida, Saccharomyc*, Aspergill*，以及相关MeSH术语
    - **代谢物术语**：Metabol*, "compound*", "chemical*", "secondary metabolite*", "primary metabolite*", "metabolic product*", "short-chain fatty acid*", SCFA*, TMAO*, "bile acid*", "amino acid*", "lipid*", "carbohydr*", "polysaccharid*", "oligosaccharid*", "sugar*", "mannan*", "glycan*", "degradation", "ferment*", "digest*", "enzyme*", "catalyz*", "hydrolys*", "synthesis", "biosynthesis", "microb* metabolite*"，以及相关MeSH术语
  
  - **连接管理**：配置带连接池(pool_connections=50, pool_maxsize=50)和重试机制(MAX_RETRIES=5, backoff_factor=1.5)的会话
  
  - **批处理系统**：实现优化的批处理参数(PMID_BATCH_SIZE=800, DETAIL_BATCH_SIZE=20)以平衡吞吐量和API限制
  
  - **错误处理**：开发全面的错误检测和恢复机制，对瞬时故障进行自动重试
  
  - **元数据提取**：提取详细论文元数据，包括PMID、Title、Abstract、Article Types、Authors、Citation、First Author、Journal/Book、Publication Year、Create Date、PMCID、DOI、MeSH_Terms和Substances
  
  - **检查点机制**：实现可恢复爬取，通过检查点跟踪从中断处继续
  
  - **进度监控**：创建详细的日志和统计跟踪（完成计数、失败率、处理时间）

- **交付成果**：
  - 功能完整的PubMed爬虫系统
  - 400,000+篇论文的原始数据集（约953MB）
  - 爬取统计和元数据摘要报告
  - 数据质量验证报告

- **资源**：
  - 8+核心、32GB内存的开发服务器
  - PubMed API凭证（EMAIL, API_KEY）
  - 2GB原始数据存储空间及扩展空间

### 1.2 数据标准化管道（1周）
- **目标**：将原始文档数据转换为适合实体提取的干净、标准化格式
- **实施细节**：
  - **文本规范化**：实现使用正则表达式删除HTML标签(`re.sub(r'<.*?>', '', text)`)、标准化特殊字符(`re.sub(r'[^a-zA-Z0-9.,;:?!()\s]', ' ', text)`)、规范化空白(`re.sub(r'\s+', ' ', text.strip())`)和转换为小写的函数
  
  - **元数据标准化**：使用`pd.to_datetime`标准化日期并采用统一输出格式(%Y-%m-%d)，规范化作者名称和期刊名称
  
  - **去重系统**：开发基于PMID的去重机制，智能解决重复记录冲突，为不同字段类型使用聚合函数：
    - 文本字段：用分隔符连接(`"; ".join`)
    - 数字字段：选择最小值(`"min"`)
  
  - **字段精简**：删除不必要的列（来源、结果、标签、nihms id等），同时保留所有必要元数据
  
  - **验证检查**：实现数据验证以识别缺失或格式错误的条目
  
  - **多格式支持**：确保处理各种输入格式（CSV、JSON）并产生一致输出
  
  - **批处理**：使用tqdm进度跟踪实现大型数据集的并行处理
  
  - **日志管理**：实现全面的日志记录，包括时间戳、进程标识和错误详情

- **交付成果**：
  - 统一格式的标准化数据集
  - 数据质量报告（有效/无效记录计数、字段完整性）
  - 带字段描述的数据字典
  - 带文档的标准化管道代码

- **资源**：
  - 8+核心处理服务器
  - Python数据处理库（pandas, numpy）

### 1.3 相关性过滤系统（1周）
- **目标**：将语料库过滤为专门讨论微生物-代谢物关系的论文
- **实施细节**：
  - **微生物关键词检测**：使用定义的`microbe_keywords`列表实现全面的微生物术语检测，包括"microbe"、"bacteria"、"fungus"、"yeast"、"archaea"、"microbiome"、"microbiota"等术语
  
  - **代谢物关键词检测**：使用定义的`metabolite_keywords`列表实现全面的代谢物术语检测，包括"metabolite"、"compound"、"SCFA"、"fatty acid"、"bile acid"、"amino acid"等术语
  
  - **共现分析**：开发`contains_keywords`函数检查单个文本中关键词的存在：
    ```python
    def contains_keywords(text, keywords):
        if not isinstance(text, str):
            return False
        text_lower = text.lower()
        return any(kw in text_lower for kw in keywords)
    ```
    
    以及`record_contains_both`函数验证两种实体类型是否都出现在每篇论文中：
    ```python
    def record_contains_both(row):
        has_microbes = (contains_keywords(row['title'], microbe_keywords) or
                        contains_keywords(row['abstract'], microbe_keywords) or
                        contains_keywords(row['summary'], microbe_keywords))
        
        has_metabolites = (contains_keywords(row['title'], metabolite_keywords) or
                           contains_keywords(row['abstract'], metabolite_keywords) or
                           contains_keywords(row['summary'], metabolite_keywords))
        
        return has_microbes and has_metabolites
    ```
  
  - **上下文分析**：分析标题、摘要和总结字段中的实体提及
  
  - **文章类型过滤**：根据精心策划的排除列表过滤出不相关的文章类型
  
  - **验证抽样**：创建验证数据集(30demo.csv)，包含正对照和随机样本
  
  - **多阶段过滤**：实现初始基于关键词的过滤，然后进行更复杂的实体识别
  
  - **统计报告**：跟踪过滤统计（原始计数、过滤计数、保留百分比）

- **交付成果**：
  - 专注于微生物-代谢物关系的过滤语料库
  - 过滤统计报告（按过滤类型计数、保留百分比）
  - 手动验证样本的验证数据集
  - 带文档的过滤系统代码

- **资源**：
  - 批处理的高性能服务器
  - 优化用的Cursor编码工具

## 第2阶段：实体和关系提取（4周）

### 2.1 模型训练和微调基础设施（1周）
- **目标**：设置模型微调基础设施，降低大规模处理的API成本
- **实施细节**：
  - **GPU服务器配置**：设置A100 GPU服务器（租用基础，1个月）用于模型训练和推理
  
  - **深度学习框架设置**：安装和配置带适当CUDA支持的PyTorch/Transformers
  
  - **训练数据准备管道**：
    - 使用基于API的提取处理一小部分（约5,000篇文章）
    - 手动验证更小子集（100-200篇文章）用于评估
    - 将API提取结果转换为训练样例
    - 创建数据集分割（训练/验证/测试）
  
  - **模型选择**：评估资源高效的候选基础模型：
    - Llama 3 8B
    - Mistral 7B
    - Falcon 7B
    - 其他开源生物医学LLMs

  - **参数高效微调**：实现LoRA/QLoRA微调以最小化内存需求：
    - 配置适配器秩和alpha参数
    - 设置梯度累积以获得有效批量大小
    - 实现混合精度训练
    - 配置学习率和权重衰减
  
  - **量化优化**：实现用于推理的8位和4位量化：
    - 测试各种量化方案（动态/静态）
    - 评估准确性影响
    - 优化内存效率
  
  - **评估管道**：实现自动评估系统：
    - 精确率、召回率和F1指标
    - 与API生成结果的比较
    - 实体和关系类型细分

- **交付成果**：
  - 可运行的GPU训练基础设施
  - 训练数据准备管道
  - 模型微调脚本
  - 评估框架
  - 微调实验跟踪系统

- **资源**：
  - A100 GPU（租用基础，1个月）
  - 16+核心CPU服务器
  - 64GB+内存
  - 50GB+用于模型权重和数据集的SSD存储
  - HuggingFace Transformers库
  - 用于参数高效微调的PEFT库
  - 用于实验跟踪的Weights & Biases或TensorBoard

### 2.2 实体提取系统（1周）
- **目标**：构建全面系统，提取带详细属性的微生物和代谢物实体
- **实施细节**：
  - **混合处理系统**：实现双轨处理架构：
    - 用于开发和原型验证的API轨道
    - 用于大规模处理的本地模型轨道
    - 基于样本复杂性和资源可用性的动态路由
  
  - **API客户端架构**：实现具有多层优化的`BiomedAPIClient`类：
    - 连接管理（重试策略、backoff_factor、池配置）
    - 多级缓存（memory_cache和disk_cache）
    - 错误处理（status_forcelist、异常处理）
    - 请求优化（超时配置、会话重用）
  
  - **本地模型管理器**：实现用于高效模型推理的`LocalModelManager`类：
    - 带量化的模型加载（8位/4位）
    - 基于GPU内存的批量大小优化
    - 结果缓存和去重
    - 资源监控和管理
  
  - **提取提示工程**：实现请求结构化提取的详细提示模板：
    ```
    从这篇文献中提取生物医学实体为结构化JSON：
    PMID: {pmid}
    标题: {title}
    摘要: {abstract}
    总结: {summary}
    
    返回格式：带'pmid'、'microbes'、'metabolites'和'experimental_conditions'字段的JSON。
    
    对于微生物包括：
      - name: 全名
    - taxonomy: domain, phylum, class, order, family, genus, species, strain
    - source: body_part, environment, is_human_associated
    - identifiers: ncbi_taxid, bacdive_id
    
    对于代谢物包括：
    - name: 全名
    - identifiers: kegg, hmdb, cas, chebi, smiles, inchi
    - properties: role, molecular_weight, is_secondary
    
    对于实验条件包括：
    - experimental_subjects: type, name, characteristics, quantity, source
    - methods: name, description, purpose
    - equipment: name, model
    - controls: description
    - treatments: agent, dose, duration, frequency
    - physical_parameters: temperature, ph, medium    
    对于缺失信息使用"none"。仅返回有效、格式正确的JSON。
    ```
  
  - **响应解析和验证**：开发`_parse_response`、`_extract_partial_data`和`_validate_entity_structure`方法处理各种响应格式并确保数据完整性
  
  - **检查点管理**：实现`CheckpointManager`类跟踪已处理的PMID并启用可恢复处理
  
  - **输出格式化**：创建`OutputManager`类管理JSONL和CSV输出，优化批量写入
  
  - **工作管理**：实现`WorkerManager`类根据响应时间和可用资源动态调整并发
  
  - **GPU优化处理**：实现用于高效GPU推理的`process_batch_gpu`函数：
    - 基于GPU内存的动态批量大小
    - 内存高效的注意力机制
    - 高效张量管理

- **交付成果**：
  - 功能完整的实体提取系统，支持API/本地模型双轨
  - JSONL和CSV格式的提取实体数据集
  - 实体提取统计（按类型计数、处理时间）
  - 质量评估报告（样本验证）

- **资源**：
  - A100 GPU（与模型微调共享）
  - 用于开发和验证的LLM API访问
  - 上一步的微调模型
  - 16+核心64GB内存的处理服务器

### 2.3 实体消歧系统（1周）
- **目标**：开发系统使用标准本体规范化和去重提取的实体
- **实施细节**：
  - **MeSH集成**：实现MeSH树数据加载(`_load_mesh_json`、`_load_mesh_csv`、`_process_mesh_tree`)用于实体规范化
  
  - **多策略匹配**：开发带三种匹配策略的`_find_closest_mesh_match`函数：
    - 精确匹配：MeSH术语的直接查询
    - 部分匹配：实体名称包含在MeSH术语中或反之
    - 基于标记的匹配：多词实体的词级匹配
  
  - **实体特定消歧**：实现针对不同实体类型的专门消歧：
    - `disambiguate_microbe`：处理包含分类水平的微生物特定消歧
    - `disambiguate_metabolite`：处理包含化学特性的代谢物特定消歧
    - `disambiguate_experimental_subject`和`disambiguate_method`：处理实验上下文消歧
  
  - **格式特定处理**：创建`disambiguate_jsonl_entry`和`disambiguate_csv_row`函数一致处理不同数据格式
  
  - **MeSH ID和树码分配**：将实体映射到带层次信息的标准化标识符
  
  - **批处理优化**：实现内存高效的批处理用于大型实体集
  
  - **缓存管理**：实现查找缓存以加速重复消歧操作

- **交付成果**：
  - 实体消歧模块
  - 带规范形式的规范化实体数据库
  - MeSH映射统计（匹配百分比、匹配类型分布）
  - 质量评估报告

- **资源**：
  - MeSH树数据（JSON和CSV格式）
  - 8+核心处理服务器

### 2.4 关系提取系统（1周）
- **目标**：提取和表征微生物和代谢物之间的关系，进行强度评估
- **实施细节**：
  - **混合处理系统**：实现双轨架构：
    - 用于开发和复杂关系的API轨道
    - 用于大规模处理的本地模型轨道
    - 系统间基于复杂性的路由
  
  - **本地模型优化**：配置关系提取模型进行高效推理：
    - 8位量化提高内存效率
    - 批处理以最大化GPU利用率
    - 内存高效的注意力机制
  
  - **关系提示工程**：实现请求关系提取的结构化提示模板：
    ```
    基于以下提取的实体信息，分析文献中的关系：
    
    文献信息：
    PMID: {pmid}
    标题: {title}
    摘要: {abstract}
    
    已知实体：
    {formatted_entities}
    
    请分析并提取所有实体间的关系，以有效JSON数组格式返回。每个关系应包括：
    [
      {
        "subject": {
          "type": "实体类型", // 例如：微生物、代谢物
          "name": "名称",
          "id": "主标识符（例如KEGG、ncbi_taxid等）"
        },
        "predicate": {
          "verb": "关系谓词",
          "ontology_id": "GO/RO编号"
        },
        "object": {
          "type": "实体类型",
          "name": "名称",
          "id": "主标识符"
        },
        "evidence": {
          "text": "原始文本",
          "experiment": "实验方法",
          "directionality": "作用方向"
        },
        "provenance": {
          "confidence": 0.95, // 置信度分数，0-1之间小数
          "section": "results/methods" // 关系出现的文章部分
        }
      }
    ]
    
    可用的关系谓词：
    - regulates (RO:0002211) - 调节关系
    - catalyzes (GO:0003824) - 催化关系
    - inhibits (RO:0002408) - 抑制关系
    - activates (RO:0002407) - 激活关系
    - produces (RO:0002233) - 产生关系
    - interacts_with (RO:0002437) - 互动关系
    ```
  
  - **响应处理**：实现`_parse_response`、`_extract_json_from_text`和`_validate_relation`函数进行强大的响应处理
  
  - **强度评估**：基于以下因素增强关系的强度分类（强/弱/未知）：
    - 统计显著性指标（p值提及）
    - 强度关键词（"significantly"、"markedly"等）
    - 作者确定性评估
  
  - **文献影响评估**：实现期刊影响因子集成：
    - 创建期刊到影响因子的映射表
    - 从Journal/Book字段提取期刊信息
    - 计算结合多个因素的自定义影响分数
  
  - **批处理优化**：实现用于微调模型推理的批处理：
    - 基于关系复杂性的动态批量大小
    - 复杂关系的基于优先级的调度
    - 大批量的内存管理

- **交付成果**：
  - 支持API和本地模型的关系提取系统
  - 带强度评估的关系数据集
  - 影响因子映射表
  - 质量评估报告
  - 批处理性能指标

- **资源**：
  - 用于批处理的A100 GPU（与实体提取共享）
  - 用于开发和复杂关系的LLM API访问
  - 上一步的微调模型
  - 期刊影响因子数据集

## 第3阶段：知识图谱和向量数据库（3周）

### 3.1 知识图谱实现（1周）
- **目标**：设计和实现Neo4j知识图谱存储实体和关系
- **实施细节**：
  - **模式设计**：创建详细模式：
    - 节点标签：Microbe、Metabolite、Paper、Sentence
    - 节点属性：每种实体类型的综合属性集
    - 关系类型：六种标准关系类型加元数据关系
    - 关系属性：强度、置信度、证据、影响指标
  
  - **数据转换**：开发处理器将实体和关系数据转换为Neo4j兼容格式
  
  - **批量导入**：实现使用Neo4j批量导入工具的高效批量导入：
    - 基于CSV的导入以最大化效率
    - 导入大小优化以避免内存问题
    - 大型导入的事务管理
  
  - **索引创建**：设计和实现常见查询模式的索引：
    - 常见属性的节点属性索引
    - 强度和置信度的关系属性索引
    - 文本搜索的全文索引
  
  - **约束定义**：实现唯一性和存在性约束：
    - 实体标识符唯一性
    - 必需属性约束
    - 关系有效性约束
  
  - **查询模板**：开发常见操作的Cypher查询模板：
    - 实体检索查询
    - 关系模式查询
    - 基于属性的过滤查询
    - 多跳关系查询

- **交付成果**：
  - Neo4j知识图谱实例
  - 导入脚本和工具
  - 模式文档
  - 查询模板库
  - 性能基准报告

- **资源**：
  - Neo4j服务器（8+核心，32GB内存）
  - 3GB图数据存储
  - Python Neo4j驱动

### 3.2 向量数据库实现（1周）
- **目标**：实现Milvus向量数据库用于语义搜索能力
- **实施细节**：
  - **文档分块**：实现智能文本分块策略：
    ```python
    def chunk_text(text, chunk_size=300, overlap=50):
        words = text.split()
        chunks = []
        start = 0
        while start < len(words):
            end = start + chunk_size
            chunk = " ".join(words[start:end])
            chunks.append(chunk)
            start += (chunk_size - overlap)
        return chunks
    ```
  
  - **嵌入生成**：实现文本嵌入管道使用：
    - 适当的嵌入模型（sentence-transformers或类似）
    - 高效的批处理（可配置批量大小）
    - 相同文本块的缓存
    - 大型文档的内存高效处理
  
  - **Milvus模式设计**：创建集合模式：
    - 向量字段（嵌入，1536维）
    - 标量字段（pmid, chunk_id, text, entity_references）
    - 适当的索引类型（HNSW以获得最佳性能/内存平衡）
  
  - **索引优化**：配置和优化HNSW索引参数：
    - `M`参数（16-64）控制连接数
    - `efConstruction`（100-500）控制构建质量
    - `ef`（40-400）控制搜索质量
  
  - **资源高效导入**：实现内存高效的批量导入：
    - 基于可用内存的可配置批量大小
    - 进度跟踪和检查点
    - 错误处理和重试机制

- **交付成果**：
  - Milvus向量数据库实例
  - 文档处理和嵌入管道
  - 导入脚本和工具
  - 搜索性能基准报告
  - 索引配置文档

- **资源**：
  - Milvus服务器（8+核心，32GB内存）
  - 用于嵌入生成的A100 GPU（与其他组件共享）
  - 10GB向量数据存储
  - Python Milvus客户端

### 3.3 数据库集成和API（1周）
- **目标**：集成知识图谱和向量数据库，提供统一查询接口
- **实施细节**：
  - **交叉引用机制**：实现Neo4j和Milvus之间的双向引用：
    - 在Neo4j节点中存储向量ID
    - 在Milvus标量字段中存储图节点ID
    - 实现系统间查询函数
  
  - **统一搜索API**：开发集成图和向量搜索的API端点：
    - 带多源结果的组合查询端点
    - 结果合并和排序函数
    - 源特定过滤参数
  
  - **查询转换**：创建将自然语言查询转换为适当格式的模块：
    - 结构化查询的Cypher查询生成器
    - 语义查询的向量查询生成器
    - 复杂查询的混合查询处理器
  
  - **结果聚合**：实现机制组合和排序来自多个源的结果：
    - 基于图的结果排序
    - 基于向量相似度的排序
    - 组合评分函数
    - 结果去重
  
  - **API文档**：创建全面的OpenAPI/Swagger文档：
    - 端点描述
    - 参数规范
    - 示例请求和响应
    - 错误处理文档
  
  - **认证和速率限制**：实现基本安全功能：
    - API密钥认证
    - 按客户端速率限制
    - 请求日志和监控

- **交付成果**：
  - 集成数据库系统
  - API实现
  - API文档
  - 集成测试套件
  - 性能基准报告

- **资源**：
  - API服务器（4+核心，16GB内存）
  - FastAPI框架
  - Neo4j和Milvus的Python客户端

## 第4阶段：用户界面和部署（2周）

### 4.1 查询引擎和RAG实现（1周）
- **目标**：构建带检索增强生成的智能查询处理系统
- **实施细节**：
  - **意图分类器**：实现查询意图分类以路由到：
    - 图查询（结构化关系问题）
    - 向量搜索（语义内容问题）
    - 混合查询（同时需要两者）
  
  - **Cypher查询生成器**：开发自然语言到Cypher的转换：
    - 查询中的实体识别
    - 关系模式识别
    - 基于模板的查询构建
  
  - **向量搜索优化**：实现结合嵌入相似度和关键词过滤的混合搜索：
    - 生成查询嵌入
    - 执行初始向量相似度搜索
    - 应用基于关键词的过滤
    - 基于相关性重新排序
  
  - **上下文组装**：为RAG创建上下文组装：
    - Top-k段落检索
    - 实体上下文包含
    - 源引用跟踪
    - 按相关性排序上下文
  
  - **响应生成**：实现生成连贯答案的提示模板：
    - 问题聚焦的提示
    - 上下文集成
    - 引用包含
    - 事实验证
  
  - **结果格式化**：创建标准化响应格式：
    - 机器消费的JSON结构
    - 用户显示的格式化文本
    - 引用格式化
    - 置信度评分

- **交付成果**：
  - 查询处理引擎
  - RAG实现
  - 意图分类模型
  - 查询转换模板
  - 性能评估报告

- **资源**：
  - API服务器（与数据库集成共享）
  - 用于响应生成的LLM API访问或微调模型
  - 用于查询编码的文本嵌入模型

### 4.2 Web界面和部署（1周）
- **目标**：创建用户界面并部署完整系统
- **实施细节**：
  - **搜索界面**：实现自然语言查询界面：
    - 查询输入组件
    - 查询历史跟踪
    - 自动完成建议
    - 查询细化选项
  
  - **结果显示**：创建用于显示的组件：
    - 实体信息卡
    - 关系可视化
    - 带高亮的证据段落
    - 带链接的源引用
  
  - **图可视化**：实现交互式图形探索组件：
    - 力导向布局
    - 节点和边样式
    - 交互式过滤
    - 点击时的详情展开
  
  - **容器化**：为所有系统组件创建Docker容器：
    - 数据库容器（Neo4j, Milvus）
    - API容器
    - 前端容器
    - 模型服务容器
  
  - **配置管理**：实现基于环境的配置：
    - 环境变量配置
    - 配置文件支持
    - 密钥管理
  
  - **监控设置**：配置日志、指标和警报：
    - API请求日志
    - 错误跟踪
    - 性能指标
    - 资源利用率监控

- **交付成果**：
  - Web应用前端
  - Docker compose配置
  - 部署文档
  - 系统监控仪表板
  - 用户指南

- **资源**：
  - 前端框架（React/Vue）
  - Docker及相关工具
  - Web服务器
  - 监控工具（Prometheus, Grafana）

## 资源需求

### 计算资源

#### 开发环境
- **本地开发**：
  - 现代工作站（4+核心，16GB内存）
  - Git用于版本控制
  - Cursor编码工具用于AI辅助开发
  - IDE（PyCharm/VSCode）

#### 处理环境
- **数据处理服务器**：
  - 16+CPU核心
  - 64GB+内存
  - SSD存储
  - A100 GPU（租用基础，1个月）

#### 部署环境
- **数据库服务器**：
  - Neo4j服务器：8+核心，32GB内存
  - Milvus服务器：8+核心，32GB内存
  - 总计30GB存储（最低）

- **API服务器**：
  - 4+核心，16GB内存
  - SSD存储

### 软件需求
- **编程语言**：
  - Python 3.8+（主要语言）
  - JavaScript/TypeScript（前端）

- **框架和库**：
  - PyTorch/Transformers（模型微调）
  - PEFT（参数高效微调）
  - FastAPI（API开发）
  - Neo4j Python驱动
  - PyMilvus
  - Pandas, NumPy（数据处理）
  - React/Vue（前端）

- **基础设施**：
  - Docker和Docker Compose
  - Git
  - Prometheus和Grafana（监控）

### 外部服务
- **API访问**：
  - PubMed API凭证
  - LLM API访问（用于开发和验证）

## 时间线和里程碑

| 里程碑 | 描述 | 时间线 | 依赖项 |
|-----------|-------------|----------|--------------|
| M1.1 | 数据获取完成 | 第1周 | 无 |
| M1.2 | 数据标准化完成 | 第2周 | M1.1 |
| M1.3 | 相关性过滤完成 | 第3周 | M1.2 |
| M2.1 | 模型训练基础设施完成 | 第4周 | M1.3 |
| M2.2 | 实体提取完成 | 第5周 | M2.1 |
| M2.3 | 实体消歧完成 | 第6周 | M2.2 |
| M2.4 | 关系提取完成 | 第7周 | M2.3 |
| M3.1 | 知识图谱实现 | 第8周 | M2.4 |
| M3.2 | 向量数据库实现 | 第9周 | M2.4 |
| M3.3 | 数据库集成 | 第10周 | M3.1, M3.2 |
| M4.1 | 查询引擎与RAG | 第11周 | M3.3 |
| M4.2 | Web界面与部署 | 第12周 | M4.1 |

**总时间线：12周（3个月）**

## 风险管理

### 技术风险
- **数据量挑战**：
  - *风险*：处理400,000+篇论文可能超出计算资源
  - *缓解*：实现基于检查点的批处理，使用A100 GPU加速，优化内存使用

- **API速率限制和成本**：
  - *风险*：PubMed或LLM API速率限制和成本可能限制开发
  - *缓解*：实现带指数退避的复杂重试，多级缓存，并加速向微调模型过渡

- **模型微调资源需求**：
  - *风险*：微调大型模型可能超出可用GPU资源
  - *缓解*：使用参数高效微调（LoRA/QLoRA），更小的基础模型（7-8B参数），和量化技术

- **模型性能vs API质量**：
  - *风险*：微调模型可能不匹配API质量
  - *缓解*：混合方法，对简单情况使用微调模型，对复杂情况使用API，持续模型评估和改进

### 进度风险
- **处理时间超出**：
  - *风险*：批处理可能比估计时间更长
  - *缓解*：早期优化，实现并行处理，优先处理最有价值的子集进行初始处理

- **模型训练复杂性**：
  - *风险*：模型微调可能需要比计划更多的迭代
  - *缓解*：从更简单的任务开始，使用来自类似领域的迁移学习，实现自动超参数优化

- **集成挑战**：
  - *风险*：组合组件可能揭示不可预见的问题
  - *缓解*：清晰的接口定义，持续集成测试，组件隔离的模块化设计

### 资源约束
- **GPU可用性和成本**：
  - *风险*：A100 GPU租用成本可能超出预算或可用性可能有限
  - *缓解*：优化GPU使用计划，探索替代GPU（A10G, RTX 4090等），实现GPU任务的高效调度

- **开发时间约束**：
  - *风险*：实施时间可能受学术日程限制
  - *缓解*：优先核心功能，实施MVP方法，尽可能利用现有库和工具

### 缓解策略
- 每周进度审查和资源分配调整
- 关键组件的早期原型测试
- 可能的并行开发轨道
- 定期数据质量评估
- 混合API/本地模型方法平衡质量和资源使用
- 持续优化资源密集型流程

# BioRelation平台创新改进计划

## 一、动态适应性爬虫系统

### 设计细节
1. 实现基础爬虫框架，使用现有plan.md中的PubMed爬取基础架构
2. 添加文献相关性评分模块，基于以下特征计算分数：
   - 微生物和代谢物关键词密度
   - 关系术语出现频率
   - MeSH术语匹配度
3. 设计查询修改器，根据相关性分数自动调整搜索参数：
   - 扩展/缩小关键词范围
   - 调整布尔运算符组合
   - 根据高分文档添加新术语
4. 实现批处理反馈循环，每批次处理后优化下一批次查询
5. 构建日志和监控系统，追踪查询演化和效果变化

### 实施路径
1. 在`./src/data/crawlers/`目录下创建`adaptive_crawler.py`
2. 扩展现有`pubmed_crawler.py`，添加自适应组件
3. 创建`relevance_scorer.py`实现评分算法
4. 实现`query_optimizer.py`处理查询调整逻辑
5. 更新配置文件支持自适应参数

## 二、分层过滤策略

### 设计细节
1. 设计三级过滤架构：
   - 第一级：基于关键词和规则的快速过滤（高召回率）
   - 第二级：轻量级ML模型过滤（平衡精确率和召回率）
   - 第三级：深度学习模型精确过滤（高精确率）
2. 实现统一的分层处理管道，支持跳过特定层次
3. 为每个过滤层级创建独立评估指标
4. 设计动态阈值调整机制，根据后续处理能力调整过滤严格度

### 实施路径
1. 在`./src/preprocessing/`目录下创建`tiered_filter.py`
2. 实现`./src/preprocessing/filters/`目录，包含三个过滤器模块
3. 创建配置文件定义每层过滤参数和阈值
4. 实现过滤评估和报告功能

## 三、领域特定预训练策略

### 设计细节
1. 收集生物医学领域文献语料库，包括但不限于已获取PubMed文章
2. 设计领域特定词汇表扩充现有基础模型词表
3. 实现轻量级预训练流程，专注于生物医学文献的语言结构
4. 使用掩码语言模型和文档级表示学习目标
5. 设计资源高效的训练计划，最大化利用有限GPU资源

### 实施路径
1. 在`./src/models/`创建`domain_pretraining/`目录
2. 实现`corpus_preparation.py`处理预训练语料库
3. 创建`vocabulary_augmentation.py`扩展现有词表
4. 实现`efficient_trainer.py`执行资源优化预训练
5. 设计评估脚本`domain_evaluation.py`

## 四、联合实体关系提取

### 设计细节
1. 设计端到端架构，同时提取实体和关系：
   - 共享编码器层处理输入文本
   - 实体识别解码器识别微生物和代谢物
   - 关系分类解码器预测实体间关系
2. 实现联合损失函数，平衡实体识别和关系提取
3. 设计特殊提示模板，引导模型同时关注实体和关系
4. 创建适应联合提取的数据处理管道

### 实施路径
1. 在`./src/extraction/`目录下创建`joint_extractor.py`
2. 实现`joint_prompts.py`定义联合提取提示模板
3. 创建`joint_processor.py`处理输入/输出格式
4. 设计`joint_training.py`实现训练流程
5. 添加评估脚本`joint_evaluation.py`

## 五、混合图谱结构

### 设计细节
1. 扩展Neo4j模式支持超边（表示多实体关系）：
   - 创建关系节点表示复杂关系
   - 设计连接主实体和关系节点的边结构
2. 实现用于创建和查询超图结构的API扩展
3. 开发图形化表示复杂关系的可视化组件
4. 创建专用查询语言转换器支持超图查询

### 实施路径
1. 在`./src/knowledge_graph/`目录下更新`schema_design.py`
2. 创建`hypergraph_extension.py`实现超图功能
3. 扩展`graph_api.py`支持超图操作
4. 在`./src/api/`目录下更新查询处理器

## 六、图神经网络增强

### 设计细节
1. 实现基于PyTorch Geometric的GNN模型：
   - 使用GraphSAGE或GAT架构编码节点特征
   - 设计合适的消息传递机制
2. 创建基于Neo4j图数据的GNN训练管道
3. 实现关系预测和路径推荐功能
4. 设计增量训练机制，随图谱增长更新模型

### 实施路径
1. 在`./src/models/`目录下创建`gnn/`子目录
2. 实现`graph_encoder.py`定义GNN架构
3. 创建`relation_predictor.py`预测潜在关系
4. 实现`neo4j_dataset.py`从图数据库加载训练数据
5. 添加`gnn_trainer.py`处理训练流程

## 七、多模态查询界面

### 设计细节
1. 设计支持多种查询方式的前端界面：
   - 文本查询组件（自然语言）
   - 图形查询构建器（可视化方式）
   - 混合查询编辑器
2. 实现查询转换层，将各类查询转为标准格式
3. 创建查询验证和建议系统
4. 设计响应式界面适应不同设备

### 实施路径
1. 在`./src/frontend/`目录创建`components/query/`子目录
2. 实现`TextQueryInput.js`文本查询组件
3. 创建`GraphQueryBuilder.js`图形查询构建器
4. 实现`QueryConverter.js`处理转换逻辑
5. 更新`ApiService.js`支持多模态查询请求

## 八、交互式知识探索

### 设计细节
1. 设计基于D3.js的交互式图形可视化：
   - 支持动态节点展开/折叠
   - 实现关系过滤和高亮
   - 添加缩放和拖拽功能
2. 创建增量加载机制，按需获取数据
3. 实现细节视图和概览切换
4. 添加导出和分享功能

### 实施路径
1. 在`./src/frontend/`目录创建`components/visualization/`子目录
2. 实现`KnowledgeExplorer.js`主可视化组件
3. 创建`GraphControls.js`界面控制组件
4. 实现`DetailView.js`显示实体和关系详情
5. 添加`DataLoader.js`处理增量数据获取

## 九、对照实验设计

### 设计细节
1. 选择100篇高质量论文作为评估基准：
   - 手动标注实体和关系
   - 创建标准化评估格式
2. 实现自动评估流程，计算精确率、召回率和F1分数
3. 设计多维度评估指标（实体类型、关系类型）
4. 创建可视化错误分析工具

### 实施路径
1. 在`./src/evaluation/`目录下创建`gold_standard/`子目录存放标准数据
2. 实现`benchmark_creator.py`生成评估基准
3. 创建`auto_evaluator.py`执行自动评估
4. 实现`error_analyzer.py`提供错误分析功能
5. 添加`visualization.py`呈现评估结果

## 十、用户反馈循环系统

### 设计细节
1. 设计前端反馈收集组件：
   - 在结果显示中添加反馈按钮
   - 创建简化的错误报告表单
   - 实现正确答案提交界面
2. 开发反馈存储和处理系统
3. 创建基于用户反馈的模型更新管道
4. 实现简单的用户激励机制（贡献排行榜等）

### 实施路径
1. 在`./src/frontend/components/`创建`feedback/`子目录
2. 实现`FeedbackButton.js`和`FeedbackForm.js`
3. 在`./src/api/`下创建`feedback_handler.py`
4. 实现`./src/models/feedback_trainer.py`处理模型更新
5. 添加`./src/dashboard/feedback_analytics.py`分析反馈数据

# BioRelation项目计划

## 项目目标

开发一个能够从生物医学文献中提取、结构化和查询微生物-代谢物关系的知识图谱系统。该系统将处理大量PubMed文献，识别微生物和代谢物实体及其关系，并提供直观的查询和探索接口。

## 项目架构

系统将采用模块化架构，包括以下主要组件：

1. **数据获取模块**
   - PubMed文献爬取
   - 文献预处理和过滤
   - 数据规范化

2. **实体识别模块**
   - 微生物实体提取
   - 代谢物实体提取
   - 实体链接和标准化

3. **关系提取模块**
   - 微生物-代谢物关系识别
   - 关系属性提取
   - 文献证据关联

4. **知识存储模块**
   - 知识图谱数据库
   - 向量数据库
   - 原始文献存储

5. **查询引擎**
   - 图查询处理
   - 语义搜索
   - 查询扩展和优化

6. **用户界面**
   - 查询输入界面
   - 结果可视化
   - 知识图谱探索

## 技术栈

- **编程语言**：Python
- **数据处理**：Pandas, NumPy, SciPy
- **NLP工具**：Hugging Face Transformers, spaCy
- **数据库**：Neo4j (图数据库), Milvus (向量数据库), SQLite (关系数据库)
- **Web框架**：FastAPI, Flask
- **前端**：React, D3.js
- **容器化**：Docker

## 工作分解

### 1. 数据获取

#### 1.1 PubMed文献爬取
- 设计PubMed查询策略，优化微生物和代谢物术语组合
- 实现PubMed API客户端，处理批量请求和速率限制
- 开发断点续传和错误恢复机制
- 实现并行爬取以提高效率
- 保存原始XML和元数据

#### 1.2 数据预处理
- 解析PubMed XML，提取结构化数据
- 标准化文本（统一大小写、移除特殊字符等）
- 分句和分段
- 去除重复和不相关文献

#### 1.3 相关性过滤
- 实现基于关键词的过滤
- 开发共现分析以识别包含微生物和代谢物的段落
- 使用基础模型评估相关性分数
- 过滤不相关文献

### 2. 实体识别

#### 2.1 微生物实体识别
- 收集微生物词典和本体
- 微调NER模型识别微生物提及
- 实现规则基础的微生物识别补充
- 验证和评估微生物NER性能

#### 2.2 代谢物实体识别
- 收集代谢物词典和本体
- 微调NER模型识别代谢物提及
- 实现化学式和结构识别
- 验证和评估代谢物NER性能

#### 2.3 实体链接和消歧
- 将微生物提及链接到标准数据库(NCBI分类)
- 将代谢物提及链接到标准数据库(KEGG, ChEBI)
- 处理实体同义词和变体
- 实现实体共指消解

### 3. 关系提取

#### 3.1 关系分类模型
- 定义微生物-代谢物关系类型(产生、消耗、调节等)
- 收集和标注关系训练数据
- 微调关系分类模型
- 评估关系分类性能

#### 3.2 关系属性提取
- 提取关系方向性
- 识别关系强度和置信度指标
- 提取方法和条件信息
- 提取定量信息

#### 3.3 证据关联
- 关联关系到来源文献段落
- 提取支持证据文本
- 计算文献可信度得分
- 关联相似证据

### 4. 知识存储

#### 4.1 知识图谱设计
- 设计实体和关系模式
- 定义属性和元数据字段
- 规划索引策略
- 设计查询模式

#### 4.2 向量数据库设计
- 为文本片段生成向量表示
- 实现高效向量搜索
- 关联向量与图谱节点
- 优化相似度计算

#### 4.3 集成存储
- 在存储层集成知识图谱和向量数据
- 实现跨数据库查询
- 设计缓存策略
- 确保数据一致性

### 5. 查询引擎

#### 5.1 图查询处理
- 实现关系路径查询
- 支持复杂条件过滤
- 实现查询优化
- 处理大规模图数据

#### 5.2 语义搜索
- 将用户查询转换为向量表示
- 实现混合检索(向量+关键词)
- 对结果进行相关性排序
- 实现查询理解和重写

#### 5.3 检索增强
- 将检索结果与大型语言模型集成
- 使用检索到的内容生成综合回答
- 提供指向原始数据的证据链接
- 处理复杂和多步骤问题

### 6. 用户界面

#### 6.1 查询界面
- 设计用户友好的查询输入
- 支持自然语言和结构化查询
- 提供查询建议和自动完成
- 实现查询历史管理

#### 6.2 结果展示
- 设计清晰的结果展示格式
- 支持多种视图(表格、摘要、详细)
- 实现证据和引用显示
- 支持结果导出

#### 6.3 知识图谱可视化
- 实现交互式图可视化
- 支持节点展开和过滤
- 实现缩放、平移和搜索
- 设计视觉编码(颜色、大小、形状)

## 里程碑和时间线

| 里程碑 | 描述 | 时间线 | 依赖项 |
|-----------|-------------|----------|--------------|
| M1.1 | 数据获取完成 | 第1周 | 无 |
| M1.2 | 数据标准化完成 | 第2周 | M1.1 |
| M1.3 | 相关性过滤完成 | 第3周 | M1.2 |
| M2.1 | 模型训练基础设施完成 | 第4周 | M1.3 |
| M2.2 | 实体提取完成 | 第5周 | M2.1 |
| M2.3 | 实体消歧完成 | 第6周 | M2.2 |
| M2.4 | 关系提取完成 | 第7周 | M2.3 |
| M3.1 | 知识图谱实现 | 第8周 | M2.4 |
| M3.2 | 向量数据库实现 | 第9周 | M2.4 |
| M3.3 | 数据库集成 | 第10周 | M3.1, M3.2 |
| M4.1 | 查询引擎与RAG | 第11周 | M3.3 |
| M4.2 | Web界面与部署 | 第12周 | M4.1 |

**总时间线：12周（3个月）**

## 风险管理

### 技术风险
- **数据量挑战**：
  - *风险*：处理400,000+篇论文可能超计算资源
  - *缓解*：实现基于检查点的批处理，使用A100 GPU加速，优化内存使用

- **API速率限制和成本**：
  - *风险*：PubMed或LLM API速率限制和成本可能限制开发
  - *缓解*：实现带指数退避的复杂重试，多级缓存，并加速向微调模型过渡

- **模型微调资源需求**：
  - *风险*：微调大型模型可能超出可用GPU资源
  - *缓解*：使用参数高效微调（LoRA/QLoRA），更小的基础模型（7-8B参数），和量化技术

- **模型性能vs API质量**：
  - *风险*：微调模型可能不匹配API质量
  - *缓解*：混合方法，对简单情况使用微调模型，对复杂情况使用API，持续模型评估和改进

### 数据风险
- **数据质量变动**：
  - *风险*：PubMed数据格式和质量可能随时间变化
  - *缓解*：实现健壮的解析器，增加数据校验，定期更新爬虫

- **实体歧义性**：
  - *风险*：微生物和代谢物名称存在高度歧义性和变化
  - *缓解*：使用多种消歧技术，结合外部知识库，实现上下文感知识别

### 项目风险
- **范围蔓延**：
  - *风险*：项目范围可能随时间扩大
  - *缓解*：明确定义MVP，使用敏捷方法，优先级管理，定期回顾

- **依赖协调**：
  - *风险*：多个模块依赖可能导致延迟
  - *缓解*：建立清晰依赖图，使用契约驱动开发，关键路径管理，并行任务识别

## 资源需求

### 人员
- 项目管理
- 数据科学家
- NLP专家
- 知识图谱工程师
- 前端开发者
- 生物信息学专家（顾问）

### 技术
- 计算资源：多核CPU服务器，A100 GPU（至少一个月）
- 存储资源：至少1TB SSD
- 开发环境：Git, Docker, CI/CD管道
- 云服务：按需进行培训和部署

### 外部依赖
- PubMed API凭证
- LLM API访问（用于开发和验证）

## 项目进展更新

### M1.1 数据获取阶段完成

我们已成功完成项目的第一个里程碑（M1.1 数据获取完成），实现了以下关键成果：

#### 完成工作
1. **PubMed爬虫系统开发**
   - 设计和实现了模块化的PubMed爬虫系统，包含五个功能阶段：
     - 系统初始化与配置
     - 文献检索与PMID获取
     - 文献详情获取与解析
     - 自适应爬取与相关性评分
     - 命令行接口与执行模式
   - 开发了核心技术组件：
     - API客户端模块，支持速率限制和请求重试
     - 查询构建模块，支持层次化查询策略
     - 解析与提取模块，解析XML响应并提取文献元数据
     - 断点续传系统，支持长时间任务中断恢复
     - 灵活的命令行接口，支持多种执行配置

2. **查询策略优化**
   - 解决了原始查询过于宽泛的问题（从290万篇优化至约40万篇）
   - 实现了三种互补的检索策略：
     - 核心层查询：使用精确术语获取高相关文献
     - 扩展层查询：使用广泛术语补充潜在相关文献
     - 自适应爬取：动态优化查询，提高精确度和召回率
   - 删除或限制了高噪声术语（compound*, chemical*, enzyme*等）
   - 增加文章类型过滤，排除非研究类文章和病毒研究

3. **性能和可靠性优化**
   - 实现了批量处理和增量存储，减少内存占用
   - 开发了POST请求方式解决长查询字符串问题
   - 针对不同API端点使用不同的批次大小（PMID=800，详情=20）
   - 实现了安全的XML解析功能，处理复杂文献结构
   - 完善了错误恢复和重试机制

#### 自适应爬虫特点
自适应爬虫是我们的关键创新，其工作方式与传统查询的区别在于：

1. **动态优化**：自适应爬虫从初始查询（通常是核心查询）开始，通过分析已获取文献的相关性，动态调整查询条件。

2. **相关性评分**：集成了文献相关性评分功能，结合关键词密度、关系术语分析、MeSH术语分析和TF-IDF相似度，提供0-1分的相关性评分。高于阈值（默认0.7分）的文献被视为高相关性。

3. **过滤功能**：能过滤低相关性文献，为下一阶段数据处理提供高质量输入。系统会单独保存高相关性文献，并生成相关性分析报告。

4. **与层次化查询的关系**：自适应爬虫与层次化查询（核心层、扩展层）互为补充。层次化查询提供静态但可预测的结果，自适应爬虫提供动态优化但初始结果可能较不稳定。

#### 完成的文件
- `docs/M1.1_data_acquisition.md`：数据获取阶段完整文档
- `src/data/crawlers/pubmed_crawler.py`：基础爬虫实现
- `src/data/crawlers/adaptive_crawler.py`：自适应爬虫实现
- `src/data/crawlers/cli.py`：命令行接口
- `scripts/run_crawler.py`：主执行入口
- `config/pubmed/crawler_config.py`：爬虫配置和查询策略

#### 风险缓解成果
成功缓解了计划中的多项风险：
- **API速率限制风险**：通过实现速率限制控制和断点续传系统成功缓解
- **数据量处理风险**：通过批量处理、增量存储和断点机制有效管理
- **查询精确度风险**：通过层次化查询和自适应爬取显著提高

### 下一阶段规划（M1.2 数据标准化）
随着数据获取阶段的完成，我们即将开始M1.2数据标准化阶段，主要工作包括：
- 使用爬虫系统获取一批测试数据（约5000篇文献）
- 开发数据清洗流程，标准化文档格式和字符编码
- 实现PMID去重和元数据规范化
- 开发基于包含/排除标准的文章过滤机制
